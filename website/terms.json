{
    "NER (Named Entity Recognition)": "задача NLP, заключающаяся в распознавании именованных сущностей в текстах и их классификации.",
    "Векторизация": "процесс нахождения векторов для слов или предложений.",
    "Дистрибутивная гипотеза": "гипотеза, согласно которой, слова, часто встречающиеся в похожих контекстах, имеют схожее значение.",
    "Закон Ципфа": "закономерность естественного языка, показывающая, что если все слова текста упорядочить по убыванию частотности их использования, то частотность n-го слова в таком списке окажется приблизительно обратно пропорциональной его порядковому номеру n",
    "Корпус": "набор текстовых данных, состоящий из отдельных текстов-документов.",
    "Лемма": "начальная форма слова.",
    "Лемматизация": "процесс приведения слов к их начальным формам.",
    "Метрика": "показатель для оценки качества работы модели.",
    "Модель": "некоторый способ описания мира или какого-то явления, позволяющий представить и обобщить информацию о некоторых объектах по их признакам.",
    "Н-граммы": "последовательность слов длины н.",
    "Нейронная сеть (многослойный перцептрон, MLP)": "это последовательность различных преобразований данных, на каждом этапе которых извлекаются признаки другого порядка.",
    "Обратное распространение ошибки, backpropagation": "метод подсчёта грандиента от выходы модели к её входу.",
    "Перплексия": "метрика оценивания работы генеративной текстовой модели, которая позволяет оценить, как хорошо модель выучила язык. Формально, это обратная вероятность тестового набора, нормализованная на количество слов в тестовом корпусе слов.",
    "Полносвязный слой": "слой, в котором каждый элемент входных данных связан с каждым элементов выходных.",
    "Предобработка текста": "процесс приведения тектсовых данных к удобному для рабооты виду путем удаления пунктуации, поиска начальной формы слова и других этапов.",
    "Регулярные выражения": "способ поиска и замены текста при помощи строк определенных символов.",
    "Стемминг": "процесс нахождения основы слова.",
    "Стоп-слова": "это часто используемые слова, которые не вносят никакой дополнительной информации в текст, часто к ним относят служебные части речи, местоимения, модальные глаголы и другие частотные слова.",
    "Сэмплирование": "выбор следующего токена из множества возможных для генерации текста.",
    "Таргет": "значение, которое требуется предсказать в результате работы модели.",
    "Тестовые данные": "данные, которые модель не видела при обучении и на которых происходит оценивание качества обучения.",
    "Токен": "текстовая единица, например, слово, словосочетание, предложение и т.д.",
    "Токенизация": "процесс разбиения текста на текстовые единицы, чаще всего слова (токены).",
    "Тренировочные данные": "данные, на которых модель обучается.",
    "Функция потерь (loss function, objective)": "функция, которая оценивает, насколько сильно предсказания модели отличаются от реальных.",
    "Языковая модель": "модель, которая оценивает естественность полученной последовательности слов."
}