# Наивный байесовский классификатор

**Классификация текста** - одна из популярных задач в обработке текстовых данных. Например, определение спама в почтовом ящике, тематику поста в социальной сети, автора литературного произведения, эмоциональной окраски текста и тд. В результате обучения классификации текста модель должна выдать класс, к которому принадлежит объект, из всевозможных классов.

Текстовые данные в задачах подобного типа называются *документами.* Ответом на задачу является класс объекта или вероятности, что документ относится к некоторому классу. Таким образом, предсказанный класс - это класс с наибольшей вероятностью.

Задачу классификации можно решать привычным методом: представить текст в виде чисел, например, с помощью Bag-of-words или Tf-Idf, а дальше обучить модель классификации. Однако в этой главе мы рассмотрим другой способ, основанный на генерации, а точнее, какой текст может быть сгенерирован для некоторого класса. То есть, модель возвращает такой класс, у которого больше вероятность сгенерировать текст, подобный данному.

## Naive Bayes

Данная модель основана на *наивном* представлении того, как в тексте взаимодействуют слова. В результате работы модель выдаёт класс, который с наибольшей вероятностью относится к тексту. При этом данную вероятность можно представить немного другой формулой, основываясь на формуле Байеса:

$$
\hat{c} = argmaxP(c|d) = argmax \frac{P(d|c)*P(c)}{P(d)}
$$

То есть класс $\hat{c}$  - это класс, к которому относится текст. При этом, можно заметить, что мы считаем вероятность принадлежности документа к каждому классу, следовательно, знаменатель $P(d)$ можно опустить, так как он будет одинаков для каждого из классов:

$$
\hat{c} = argmax P(d|c) * P(c)
$$

Формула объясняется таким образом: мы берём вероятность того, что документ *d* мог быть сгенерирован для данного класса *c* и умножаем на вероятность самого класса.

Однако, нам нужно каким-то образом представить документ, чтобы высчитывать вероятности. Вот тут возникают да упрощения, из-за которых данный классификатор называется наивным. Для начала надо вспомнить, что документ - это не что иное, как список слов. 

1. Будем считать, что для определения класса нам не важна позиция слова в текста, а только его наличие и количество. Так для текста по биологии на неважно: *хвост собаки* или *собаки хвост*, нам важно, что есть слова *хвост* и *собака*. Поэтому документ можно таким образом легко представить как Bag-of-Words.
2. Будем считать, что слова не зависят друг от друга, что значит, что вероятность встретить два слова в документе равно произведению вероятности встретить каждое слово по отдельности.

Таким образом, формула превращается в:

$$
\hat{c} = argmax\;P(w_1, w_2, ..., w_k|c)*P(c) = argmax\;P(c)*\Pi_i^{k}P(w_i|c)
$$

То есть вероятность того, что документ принадлежит классу *c* - это произведение вероятностей того, что каждое из слов встречается в классе *c,* умноженное на вероятность класса.

Как мы уже раньше говорили, перемножение большого количества вероятностей приводит к очень маленьким числам, которые трудно считать в компьютере, поэтому вместо произведения вероятностей мы будем использовать сложение логарифмов вероятностей.

$$
\hat{c} = argmax\;P(c)*\Pi_i^{k}P(w_i|c) = argmax(\log{P(c)} + \sum_i^{k}\log{P(w_i|c)})
$$

## Процесс обучения

Попробуем выяснить, как вычислить каждую из вероятностей в формуле. Вероятность класса *c -* это то же, что и встречаемость этого класса, то есть количество документов класса *c* среди всех документов:

$$
P(c) = \frac{N_c}{N}
$$

Вероятность встретить слово $w_i$ в документах класса *c* - это количество раз, когда слово $w_i$ встречалась в документах класса *c*, среди всех слов документов класса *c*:

$$
P(w_i|c) = \frac{count(w_i, c)}{\sum_{w\in{V}}count(w, c)}
$$

При этом важно помнить, что *V* в данном случае - это словарь слов по всем документам, независимо от класса.

А что делать, если в тестовых данных встречается слово, которого не было в данном классе в обучающей выборке. Например, некто употребил слово *ужасно* в положительном отзыве (в контексте *ужасно хорошо*), хотя слово до этого присутствовало только в отрицательных отзывах. Получится, что вероятность $P('хорошо'|c)$  будет равна нулю, что обратит всю вероятность принадлежности к положительному классу в ноль, так как итоговая вероятность - это произведение. Однако можем вспомнить, что в языковом моделировании мы уже сталкивались с такой проблемой. Чтобы её решить, мы можем просто прибавлять единицу в знаменатель и числитель, что называется **сглаживание Лапласа**.

$$
P(w_i|c) = \frac{count(w_i, c) + 1}{(\sum_{w\in{V}}count(w, c)) + 1} 
$$

Если же в тестовых данных встретилось слово, которого нет даже в словаре по всем классам, то можно просто опустить эти слова, так как про них мы ничего не знаем.

- **Задание на подсчёт вероятности**
    
    Представим у нас есть три предложения:
    
    1. Маша ела кашу (класс 1)
    2. Каша сделана из гречневой крупы (класс 2)
    3. Кашу съели вчера (класс 1)
    
    Посчитайте вероятность принадлежности слов “есть”, “каша” и “Маша” к каждому из классов. Вычислите класс предложения 1. Считайте логарифм по основанию 2.
    
- **Ответ:**
    
    Вероятность встретить слово в тексте класса 1:
    $P("Есть"|1) = \frac{3}{7}$
    $P("Каша"|1) = \frac{3}{7}$
    $P("Маша"|1) = \frac{2}{7}$
    
    Вероятность встретить слово в тексте класса 2:
    $P("Есть"|1) = \frac{1}{6}$
    $P("Каша"|1) = \frac{1}{3}$
    $P("Маша"|1) = \frac{1}{6}$
    
    Класс предложения 1:
    Класс 1: $log_{c_{1}}(\frac{1}{2}) + log_{есть}(\frac{3}{7}) + log_{каша}(\frac{3}{7}) +
    log_{Маша}(\frac{2}{7}) \approx -5.25$
    Класс 2: $log_{c_{2}}(\frac{1}{2}) + log_{есть}(\frac{1}{6}) + log_{каша}(\frac{1}{3}) +
    log_{Маша}(\frac{1}{6}) \approx -7.75$
    

### Дополнение

Обучение модели зависит от того, какую именно задачу мы решаем. Поэтому тот процесс, который мы описали выше, можно модифицировать в зависимости от желаемого результата. Так, мы можем добавлять не только слова, но и биграмы, триграммы и тд, а иногда и просто сочетания букв, например, для определения языка документа. Мы можем добавлять дополнительные характеристики, такие как сайт, откуда был взят документ, или почта, с которой присылается письмо.

Можно придумать много модификаций, и каждую стоит подбирать под задачи. Это важно помнить не только для задачи классификации, но и для других задач тоже! :)

## Оценивание

С помощью Баесовской модели решается задача классификации, следовательно, метрики для оценивания качества модели - это **accuracy, precision** и **recall**.

При этом важно помнить о том, что часто решается задача многоклассовой классификации, что значит, что может потребоваться **macro-averaging** или **micro-averaging**, чтобы обобщить метрики по каждому из классов в одну метрику.

# Итог

В этой главе мы познакомились с базовой моделью классификации текстов **Наивные Баесовские модели**. Они позволяют нам с помощью привычного вероятностного подсчёта встречаемости слова в тексте определённого класса определять, с какой вероятностью принятый на вход документ относится к тому или иному классу. Класс с наибольшей вероятностью и есть предсказанный класс.