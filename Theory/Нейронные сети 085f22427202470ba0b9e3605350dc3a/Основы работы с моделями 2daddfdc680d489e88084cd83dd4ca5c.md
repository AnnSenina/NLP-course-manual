# Основы работы с моделями

### Словарь

**Модель** - некоторый способ описания мира или какого-то явления, позволяющий представить и обобщить информацию о некоторых объектах по их признакам.

**Тренировочные данные** - данные, на которых модель обучается.

**Тестовые данные** - данные, которые модель не видела при обучении и на которых происходит оценивание качества обучения.

**Функция потерь (loss function, objective)** - функция, которая оценивает, насколько сильно предсказания модели отличаются от реальных. 

**Таргет** - значение, которое требуется предсказать в результате работы модели.

**Метрика** - показатель для оценки качества работы модели.

**Перплексия (perplexity, PPL)** - обратная вероятность тестового набора, нормализованная на количество слов в тестовом корпусе слов

### Библиотеки

- **sklearn**
train_test_split - разделить данные на обучающую и тестовую выборки
LinearRegression, LogisticRegression… - различные модели, которые можно обучить для решения задачи
accuracy_score, classification_report - метрики оценки результатов работы
StandardScaler, MinMaxScaler - стандартизация данных
OneHotEncoder, LabelEncoder - подготовка категориальных данных

---

# Что такое модель?

**Модель** - это некоторый способ описания мира. Например, *кошка - это животное, которое ходит на четырёх лапах*. Данная модель хорошо описывает кошку, однако под это описание попадают и другие животные. Можно немного изменить: *кошка - это животное, которое ходит на четырёх лапах, имеет усы и хвост, а также мяукает.* Так, звучит наиболее похоже на кошку. Можно ещё усложнить и, например, добавить в определение *наличие шерсти*, однако это уже будет не так точно, ведь у нас есть кошки породы *сфинкс*. Хотя и в данной формулировке можно найти изъяны, например, если мы подумаем котах породы *мэнкс*, у которых нет хвоста. Таким образом, модель - это некоторый способ описания мира, который позволяет обобщить наши знания о нём. 

В машинном обучении **моделью** называется некоторая функция, которая в результате каких-то преобразований получает некоторый ответ: $y = f(x)$. Другими словами, эта модель пытается уловить связь между признаковым описание предмета $x$, и ответом, или же **таргетом**, $y$**.** У модели есть **обучаемые параметры $w$**, которые изменяются в процессе обучения, чтобы функция наиболее правдоподобно отражала данные. Эти параметры называются **весами** модели и, обычно, представляют собой *важность* конкретного признака для результата.

# Обучение модели

Как же вообще происходит обучение модели? Для начала следует определить задачу, которая будет решаться с помощью модели. Есть несколько типов:

1. **Регрессия** - предсказание числового ответа ($\mathbb{Y} = \mathbb{R}$)
    1. Цена квартиры по её характеристикам (количество квартир, год постройки дома, дорога до метро и тд). 
    2. Продолжительность поездки на такси в зависимости от времени поездки, точки начала и конца и тд
2. **Классификация** - предсказание класса некоторого объекта ($\mathbb{Y} = \{0, 1\}$ (бинарная) или $\mathbb{Y} = \{1, ..., K\}$ (многоклассовая) или $\mathbb{Y} = \{0, 1\}^K$ (многоклассовая с пересекающимися классами))
    1. Можно ли одобрять кредит клиенту по его кредитной истории (бинарная)
    2. К какой предметной области относится данный пост: биология, химия и тд (многоклассовая)
    3. К какой категории относится пост в социальной сети (с пересекающимися классами, так как у поста может быть несколько категорий)
3. **Ранжирование** - сортировка объектов по релевантности запросу ($\mathbb{Y}$)
    1. Ранжирование поисковой выдачи по запросу пользователя
- **Задание на определение типа задачи**
    1. Прогноз дохода парикмахерской в следующем месяце (регрессия)
    2. “Моя волна” яндекс музыки, которая предлагает песни (ранжирование)
    3. Определение жанра песни по его характеристикам (классификация)

Задача модели - сделать так, чтобы ответы функции были максимально близки к **таргету** или же реальному ответу. Для этого существует **функция потерь.** Например, для задачи регрессии нам важно, чтобы предсказание как можно меньше отличалось от таргета, другими словами, чтобы разница между таргетом и предсказанием была как можно ближе к нулю. Для этого можно считать среднюю разницу между предсказанием и таргетом по всем объектам в выборке: 

- $MAE = \frac{1}{l}\sum_{i=1}^{l}{|y_i - f(x_i)|} \rightarrow min$
- $MSE = \frac{1}{l}\sum_{i=1}^{l}(y_i - f(x_i))^{2} \rightarrow min$

По школьной программе мы помним, что задача поиска минимума сводится к вычислению производной функции по $x$. Производная показывает, в какую сторону нужно двигаться (другими словами, как изменить параметры $w$ модели), чтобы прийти к минимуму, следовательно, сократить разницу между предсказанием и таргетом. Процесс состоит из шагов: на каждом шаге считается производная, относительно которой изменяются параметры модели и делается *шаг* в сторону минимума. Такой процесс называется **градиентным спуском**. Более подробно о нём можно почитать [тут](https://medium.com/geekculture/gradient-descent-simplified-631a7ce38cb6) или [тут](https://neurohive.io/ru/osnovy-data-science/gradient-descent/).

![download.jpg](%D0%9E%D1%81%D0%BD%D0%BE%D0%B2%D1%8B%20%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D1%8B%20%D1%81%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8F%D0%BC%D0%B8%202daddfdc680d489e88084cd83dd4ca5c/download.jpg)

Так как **MAE** не дифференцируема в 0 (у неё не существует производной в точке $x=0$), она реже используется при обучении моделей, в то время как **MSE** пользуется большой популярностью, так как она гладкая, следовательно, дифференцируема. 

На данный момент мы можем сделать вывод, что обучение модели заканчивается тогда, когда мы пришли в минимум некоторой функции.

## Данные

Представим студента, который готовится к экзаменам. Ему прислали вопросы, которые будут потом на экзамене. Он может просто выучить все билеты и тогда он очень хорошо справится с экзаменом. Но получит ли он от этого какие-то знания? А что если ему достанется неприятный преподаватель, который поменяет все билеты, или хотя бы по одному вопросу в каждом билете? Тогда он не так хорошо справится с экзаменом. Значит, задача подготовки к экзаменам не столько про выучивание материала, сколько про знания и умение их применять. То же самое можно сказать и про модели.

Если мы обучим модель на некоторых данных, а потом проверим её работу на этих же данных, она справится с предсказаниями очень хорошо, ведь она уже видела эти данные. Однако ничего не известно про другие данные. Сможет ли она применить свои знания так же хорошо на новых данных, как на старых? Для того, чтобы оценивание было более качественным и независимым, данные делятся на **тренировочную выборку**, на которой модель учится решать задачу, и на **тестовую выборку**, на которой проверяется качество её работы. Обычно, данные делятся в отношении 80/20, 75/25, 70/30.

Таким образом, процесс обучения выглядит так:

![Снимок экрана 2023-09-07 в 13.47.25.png](%D0%9E%D1%81%D0%BD%D0%BE%D0%B2%D1%8B%20%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D1%8B%20%D1%81%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8F%D0%BC%D0%B8%202daddfdc680d489e88084cd83dd4ca5c/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2023-09-07_%25D0%25B2_13.47.25.png)

В итоге задача сводится к тому, чтобы максимально улучшить работу модели на тестовых данных, то есть получить максимально возможное качество.

Однако и в таком методе можно найти некоторые проблемы. Проверяя качество на тестовом датасете, мы совсем забываем про то, что при выгрузке модели в открытый доступ, ей будут приходить совсем новые данные, которые она не видела даже при тесте. То есть опять возникает проблема *новых данных*. Её решить так же просто - разделить датасет на три выборки - тренировочная, валидационная и тестовая. В таком разделении **валидационная** - это выборка, по которой мы следим за улучшениями работы модели, а **тестовая** - это выборка, которая играет роль новых данных: их модель не видела даже при оценивании. 

![Снимок экрана 2023-09-07 в 16.17.08.png](%D0%9E%D1%81%D0%BD%D0%BE%D0%B2%D1%8B%20%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D1%8B%20%D1%81%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8F%D0%BC%D0%B8%202daddfdc680d489e88084cd83dd4ca5c/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2023-09-07_%25D0%25B2_16.17.08.png)

### Типы данных

Признаки объекта можно разделить на три типа: бинарные, числовые, категориальные. 

- **Бинарные {0, 1}**
Обозначают наличие/отсутствие некоторого признака у объекта
Пример: *наличие кредитной истории, пол человека*
- **Категориальные**
Обозначают принадлежность объекта к некоторой категории
Пример: *национальность, профессия*
- **Числовые**
Обозначают числовые характеристики объекта
Пример: *рост человека, возраст, стоимость*

**Бинарные признаки** удобны тем, что их не надо никак обрабатывать, в то время как категориальные и числовые - необходимо.

**Категориальные признаки** обычно описываются буквами (*учитель, машинист, повар…*), однако для дальнейшей автоматической обработки нам удобнее привести их к числовому виду. Есть два популярных метода:

1. Label encoding
Каждой категории присваивается номер: *учитель - 1, машинист - 2, повар - 3…*
Однако такое кодирование создаёт некоторую иерархию среди категорий, например, учитель меньше, чем повар, так как учитель - 1, а повар - 3. Не всегда возможно придумать объяснение такой иерархии, поэтому, не всегда этот способ подходит.
2. One Hot Encoding
Каждая категория начинает представлять отдельный бинарный признак. Если категории было 3 (учитель, машинист, повар), то появится 3 столбца, в каждом из которых будут стоять единица, если объект принадлежит соответствующей категории, и ноль - если нет. Этот способ позволяет избегать иерархий категорий, но при этом создаёт много новых признаков (по количеству категорий), что может слишком сильно увеличить признаковое пространство.

**Числовые признаки** принято масштабировать или стандартизировать, другими словами, приводить к более маленьким значениям. Это делается для того, чтобы признаки, с очень разными значениями, например, возраст от 20 до 100, а зарплата в месяц от 20000 до 1000000, стали находится в одинаковом числовом промежутке. Существует также два способа:

1. Стандартизация 
Приводит значения к нормальному распределению со средним равном нулю и стандартным отклонением равным одному: $\frac{x - \mu}{\sigma}$
2. Нормализация
Приводит значения к числам в промежутке от 0 до 1 через вычитание минимального числа и деления на разницу между максимальным и минимальным: $\frac{x - min(X)}{max(X)-min(X)}$

# Оценивание модели

Помимо функции ошибки, которая позволяет следить нам, насколько хорошо идет процесс обучения, нам нужна функция, которая сможет оценивать итоговый результат работы модели на новых, как мы уже раньше сказали, тестовых данных. Важно помнить, что метрика ≠ функция ошибки, так как **метрика** - это внешняя оценка качества модели, не зависящая от параметров модели. 

Иногда метрика и функция ошибки совпадают. Например, в задачах регресии MSE используется и как функция ошибки, и как метрика качества. А вот в задачах классификации в качестве функции потерь выступает, например, кросс-энтропия (более подробно разберём после, а пока можно почитать [тут](https://wandb.ai/wandb_fc/russian/reports/---VmlldzoxNDI4NjAw#:~:text=%D0%A4%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F%20%D0%BF%D0%BE%D1%82%D0%B5%D1%80%D1%8C%20%D0%BF%D0%B5%D1%80%D0%B5%D0%BA%D1%80%D0%B5%D1%81%D1%82%D0%BD%D0%BE%D0%B9%20%D1%8D%D0%BD%D1%82%D1%80%D0%BE%D0%BF%D0%B8%D0%B8%20%E2%80%93%20%D1%8D%D1%82%D0%BE,%2C%20%D0%B3%D0%B4%D0%B5%200%20%E2%80%93%20%D0%B8%D0%B4%D0%B5%D0%B0%D0%BB%D1%8C%D0%BD%D0%B0%D1%8F%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C.)), а в качестве метрики - *accuracy,* которая считает долю правильно предсказанных ответов. 

Методы оценивания также могут делить на:

- **внешняя оценка**
Внешняя оценка предполагает сравнение результатов нескольких моделей и выбор наилучшей среди них. В случае н-грамм, можно создать несколько моделей, основанных на разном количестве слов в н-грамме, и посмотреть, какая модель лучше справляется с генерацией текста.
*Этот метод является предпочтительным, однако тяжелые нейронные сети, которые обучаются несколько дней, а то и недель, требуют больших ресурсов, то есть для такой оценки понадобится слишком много времени.*
- **внутренняя оценка**
Внутренняя оценка смотрит на качество модели независимо от его задачи. Для этой оценки требуются не только тренировочные данные, но и тестовые, на которых как раз и происходит замер качества. Тестовые данные - данные, которые модель никогда не видела, то есть они не попадались ей при обучении. Проверка на тестовых данных позволяет посмотреть, насколько модели удалось понять структуру данных, а не просто запомнить выборку.
*Этот метод быстрый, но не такой точный, как внешняя оценка.*

## Классификация

### Accuracy

Итак, у нас появилась обученная модель, которая позволяет по документу определить то, к какому классу он относится. Теперь пора определить, насколько хорошо он это делает. Самая первая **метрика**, которая приходит в голову, это **accuracy** *-*  доля верно предсказанных ответов среди всех ответов, то есть в нашем случае: *сколько классов было предсказано верно среди всех объектов**.***

$$
accuracy = \frac{count([y_{true} = y_{pred}])}{N}, N-все \;объекты
$$

Однако у такой метрики есть один недочет: она зависит от соотношения классов. Представим, что у нас выборка из 1000 человек, из которых 950 здоровых и 50 больных гриппом человек. Пусть здоровый человек будет иметь класс 0, а больной 1. Если мы сделаем константное предсказание, то есть каждому объекту будем присваивать одно и то же число, и будем считать, что у всех объектов класс 0, то тогда:

$$
accuracy = \frac{950}{1000} = 0.95
$$

Значение близко к 1, то есть очень хороший результат. Однако проблема в том, что мы упустили больных людей и не сможем вылечить их. 

![Снимок экрана 2023-09-23 в 23.17.57.png](%D0%9E%D1%81%D0%BD%D0%BE%D0%B2%D1%8B%20%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D1%8B%20%D1%81%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8F%D0%BC%D0%B8%202daddfdc680d489e88084cd83dd4ca5c/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2023-09-23_%25D0%25B2_23.17.57.png)

### Precision & Recall

Тогда вместо accuracy приходят две новые метрики: **precision** и **recall**. Но сначала давайте обсудим немного другую вещь: **confusion matrix**. Эта матрица показывает сколько предсказаний модель сделала верно или неверно относительно значений таргетов.

![Снимок экрана 2023-09-23 в 23.24.30.png](%D0%9E%D1%81%D0%BD%D0%BE%D0%B2%D1%8B%20%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D1%8B%20%D1%81%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8F%D0%BC%D0%B8%202daddfdc680d489e88084cd83dd4ca5c/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2023-09-23_%25D0%25B2_23.24.30.png)

У нас есть две группы предсказаний: верные (TP и TN) и неверные (FP и FN). Нетрудно догадаться, что True Positive (TP) и True Negative (TN) - это верно предсказанные классы (позитивный и негативный, соотвественно), а False Positive (FP) и False Negative (FN) - это ошибки модели в предсказании. 

Тогда, по этой таблице:

$$
precision = \frac{TP}{TP+FP}
$$

$$
recall = \frac{TP}{TP+FN}
$$

Более понятно **precision** и **recall** можно описать так:

- **precision** - это доля верно предсказанных значений положительного класса среди всех объектов, которым был присвоен положительный класс. Другими словами, *скольким объектам модель верно присвоила положительный класс из всех объектов, которые она посчитала положительными.*
- **recall** - это доля верно предсказанных значений положительного класса среди всех объектов, которым на самом деле должен быть присвоен положительный класс. Другими словами, *скольким объектам модель верно присвоила положительный класс из всех объектов, которые на самом деле положительные.*

В этих же обозначениях **accuracy**:

$$
accuracy = \frac{TP+TN}{TP+FP+TN+FN}
$$

- **Задание на подсчёт метрик**
    
    ![Снимок экрана 2023-09-23 в 23.47.22.png](%D0%9E%D1%81%D0%BD%D0%BE%D0%B2%D1%8B%20%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D1%8B%20%D1%81%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8F%D0%BC%D0%B8%202daddfdc680d489e88084cd83dd4ca5c/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2023-09-23_%25D0%25B2_23.47.22.png)
    
- **Ответ**
    
    ![Снимок экрана 2023-09-23 в 23.48.34.png](%D0%9E%D1%81%D0%BD%D0%BE%D0%B2%D1%8B%20%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D1%8B%20%D1%81%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8F%D0%BC%D0%B8%202daddfdc680d489e88084cd83dd4ca5c/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2023-09-23_%25D0%25B2_23.48.34.png)
    

### F1-score

Есть задачи, которым требуется иметь более высокий precision, то есть, чтобы максимальное количество предсказаний оказались верными, но при этом кого-то можно отклонить, например, *если нам нужно определить человека, которому нужно дать кредит, то лучше мы кому-то не дадим, чем дадим кому-то, кто не сможет выплатить*. Аналогично, есть задачи, для которых важнее recall, например, *когда нам нужно определить болен ли человек, мы лучше классифицируем здоровых людей, как больных, но не упустим ни одного больного.*

Однако самый лучший исход, это если у нас и recall высокий, и precision. Чтобы не следить за ними отдельно, а смотреть на их изменением одновременно, используется метрика **f1**:

$$
F_1 = \frac{2*Precision*Recall}{Precision + Recall}
$$

Можно немного модифицировать формулу, чтобы усилить влияние precision или recall:

$$
F_{\beta} = \frac{(\beta^2+1 * Precision*Recall)}{\beta^2*Precision + Recall}
$$

Если $\beta > 1$, то recall будет сильнее влиять на формулу, то есть это для случаев, когда для задачи recall важнее. Если $\beta < 1$, то важнее становится precision.

### Многоклассовая классификация

Для многоклассовой классификации в первую очередь меняется confusion matrix, так как в неё добавляются ещё столбцы и строки.

![Снимок экрана 2023-09-23 в 23.59.46.png](%D0%9E%D1%81%D0%BD%D0%BE%D0%B2%D1%8B%20%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D1%8B%20%D1%81%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8F%D0%BC%D0%B8%202daddfdc680d489e88084cd83dd4ca5c/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2023-09-23_%25D0%25B2_23.59.46.png)

Для каждого класса мы можем посчитать precision и recall так же, как и до этого, по их определению. А потом посчитать среднее этих метрик. Такой подход называется ***marcoaveraging***. Для другого подхода мы сначала считаем confusion matrix для каждого класса по отдельности. А дальше суммируем по ячейкам, получая общую confusion matrix для всех классов вместе. Тогда удобно считать метрики обычным способом.

- **Задание на подсчёт метрик 2**
    
    ![Снимок экрана 2023-09-24 в 00.26.01.png](%D0%9E%D1%81%D0%BD%D0%BE%D0%B2%D1%8B%20%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D1%8B%20%D1%81%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8F%D0%BC%D0%B8%202daddfdc680d489e88084cd83dd4ca5c/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2023-09-24_%25D0%25B2_00.26.01.png)
    

Помимо вышеупомянутых метрик существует много других, например, **AUC-ROC, Precision-Recall Curve**. Данные метрики позволяют оценить не только предсказанные классы, но и уверенность модели, если она выдаёт некоторые значения вероятности, а не только класса. О них подробнее можно почитать здесь.

## Регрессия

### Mean Squared Error (MSE) и Mean Absolute Error (MAE)

Как мы уже говорили есть две основные метрики качества регрессии: **MAE** и **MSE**.

- $MAE = \frac{1}{l}\sum_{i=1}^{l}{|y_i - f(x_i)|} \rightarrow min$
- $MSE = \frac{1}{l}\sum_{i=1}^{l}(y_i - f(x_i))^{2} \rightarrow min$

Как мы уже говорили, обычно выбирают MSE, так как её проще дифференцировать. Однако она не очень подходит для задач, в которых есть слишком большие значения - выбросы, так как она сильно штрафует за них, следовательно, подгоняет под них ответы. Это нехорошо, так как выбросы очень редки и на них не всегда есть смысл обращать внимание. MAE, наоборот, устойчива к выбросам, то есть штрафует меньше, за ошибки на очень больших значениях, а значит, более стабильна в предсказаниях. Но обе метрики измеряют ошибку в абсолютных значениях, что не всегда понятно. Например, что значит значение ошибки 10? 10 чего? Люди лучше воспринимают относительные величины, например, процент качества или изменения. Поэтому придуманы другие метрики.

### Mean Absolute Percentage Error (MAPE)

$$
MAPE = 100\% * \frac{1}{n}\sum_{i=1}^n\frac{|y_i - a(x_i)|}{|y_i|}
$$

В данном случае считается среднее отклонение от реального значения. В данном случае мы можем оценить как в среднем ошибается модель относительно реального ответа в процентах или долях.

### Symmetric Mean Absolute Percentage Error (SMAPE)

$$
SMAPE = 100\% * \frac{1}{n}\sum_{i=1}^n\frac{|y_i-a(x_i)|}{(|y_i|+|a(x_i)|) * \frac{1}{2}}
$$

Модифицированная MAPE, которая позволяет избежать 0 в знаменателе в случае, если таргет модели равен 0 ($y_i = 0$)

# Итог

Таким образом, так выглядит план работы с моделью:

1. Определение типа задачи, которую нужно решить (регрессия, классификация, генерация и тд)
2. Определение функции ошибки - функция, которую стараемся минимизировать в процессе обучения, чтобы предсказания были максимально похожи на таргет
3. Определение метрики качества - функция, которая оценивает на тестовых данных, насколько хорошо модель обобщила данные