# Языковая модель: н-граммы

**Словарь:**

**Языковая модель** - модель, которая оценивает естественность полученной последовательности слов
*language models, LMs*

**Н-граммы ***-* последовательность слов длины *н
n-grams*

**Перплексия** - метрика оценивания работы генеративной текстовой модели, которая позволяет оценить, как хорошо модель выучила язык.

**Сэмплирование** - выбор следующего токена из множества возможных для генерации текста.

Представим предложение:

(1) *Я играю на …*

Какое слово хочется вставить на место многоточия? Большинство скажут *гитаре, пианино* или даже *аккордеоне*. Возможно, кто-то скажет *площадке* или *холме у дома*. Но маловероятно, что хоть кто-то скажет *молотке* или *плечах моего друга*. Как мы можем это понять?

Мы имеем представление о том, какая последовательность слов будет наиболее ожидаема при данном контексте, а какая скорее всего вообще не может произноситься. То есть мы даём оценку вероятности появления того или иного слова или предложения в тексте или в речи. Модели, которые оценивают *естественность* или *вероятность* полученной последовательности слов, называют ***языковыми моделями* (*language models*)**. 

Как оценивается эта вероятность? Надо окунуться немного в теорию вероятности. Вероятность одного события - это произведение вероятностей маленьких событий, из которых состоит одно большое. Другими словами, это вероятность одновременного возникновения нескольких событий. Тогда вероятность предложения, это произведение вероятностей встретить каждое из слов в предложении именно в этой последовательности. Следовательно, для каждого следующего слова важно, какие именно слова шли до него. Таким образом, итоговая формула:

$$
P(sentence) = P(w_1, w_2, ..., w_n) = P(w_1)\times P(w_2|w_1)\times P(w_3|w_1, w_2)\times ...\; \times P(w_n|w_1, ..., w_{n-1})\;(1)
$$

Тогда следующий вопрос состоит в том, как нам понять, чему равен каждый множитель в этой формуле? Посчитать статистику по корпусу текстов, то есть посчитать количество раз, когда в корпусе встречается данная последовательность и её контекст.

$$
P(w_n|w_1, ..., w_{n-1}) = \frac{N(w_1, w_2, ..., w_{n-1}, w_n)}{N(w_1, w_2, ..., w_{n-1})} \; (2)
$$

Этот метод расчёта называется ***maximum likelihood estimation*** или ***MLE***. То есть таким образом мы не просто считаем количество раз, когда последовательность встретилась в корпусе, но и переводим в промежуток от 0 до 1 с помощью деления на общее количество.

И если мы возьмём предложение (1), то никаких трудностей в подсчёте вероятности не возникнет: в корпусе найдётся большое количество примеров с этим сочетанием слов. Но представим нам дано такое предложение:

(2) *Мы приехали на автобусе в радужную страну*

Встречаемость этого предложения в корпусе стремится к 0. Следовательно, и его вероятность будет равна 0. Однако вряд ли найдётся человек, который скажет, что такое предложение невозможно. Гораздо проще найти в корпусе короткие последовательности слов и оценивать их. На этом моменте мы представляем метод ***н-грамм***.

# Н-граммы

Н-граммы -  это последовательность слов определённой длины. Биграммы - последовательность длины 2, триграммы - длины 3 и т.д. Мы можем разделить предложение на всевозможные н-граммы и оценивать вероятность их появления в корпусе текстов. Например, предложение 2 можно разделить на биграммы так:

(2.1) *Мы приехали, приехали на, на автобусе, автобусе в, в радужную, радужную страну*

![Снимок экрана 2023-07-17 в 00.45.55.png](%D0%AF%D0%B7%D1%8B%D0%BA%D0%BE%D0%B2%D0%B0%D1%8F%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C%20%D0%BD-%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D1%8B%20fdfd1f68f7894d39b24314d9f07d7d73/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2023-07-17_%25D0%25B2_00.45.55.png)

Следовательно, формула расчёта вероятности предложения, поделённого на биграммы, будет:

$$
P(w_1, w_2, ..., w_n) = P(w_1)\times P(w_2|w_1)\times P(w_3|w_2)\times...\;\times P(w_n|w_{n-1})
$$

В общем виде, то есть для любого вида н-грамм, формула расчёта вероятности каждого отдельного множителя выглядит так:

$$
P(w_n|w_{1:n-1}) \approx P(w_n|w_{n-N+1:n-1}), N-количество\;слов\;в\;н-грамме
$$

Предположение, что новое слово зависит только от предыдущего (биграммы), называется ***Марковским свойством***. Таким образом, ***марковской моделью*** называется такая модель, которая предсказывает вероятность будущего слова или словосочетания на основе одного или нескольких предыдущих слов - н-граммы.

Мы уже обсудили формулу (2), которая позволяет рассчитать вероятность одного слова на основании всего предыдущего контекста. Для биграмм нам достаточно посчитать количество раз, когда данная биграмма встретилась в корпусе, и поделить на количество раз, когда встретилось только первое слово в биграмме:

$$
P(w_n|w_{n-1}) = \frac{N(w_{n-1}, w_n)}{N(w_{n-1})} \; (2.1)
$$

В общем виде формула выглядит так:

$$
P(w_n|w_{n-N+1:n-1}) = \frac{N(w_{n-N+1:n-1}, w_n)}{N(w_{n-N+1:n-1})} \; (2.2)
$$

- **Задание на расчёт вероятности предложения 1**
    
    Дано два предложения и вероятности триграмм. Посчитайте вероятность предложения *Я люблю котлетки с пюрешкой*.
    
    ![Снимок экрана 2023-07-17 в 01.03.11.png](%D0%AF%D0%B7%D1%8B%D0%BA%D0%BE%D0%B2%D0%B0%D1%8F%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C%20%D0%BD-%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D1%8B%20fdfd1f68f7894d39b24314d9f07d7d73/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2023-07-17_%25D0%25B2_01.03.11.png)
    
- **Задание на расчёт вероятности предложения 2**
    
    Дана таблица встречаемости биграмм слов, где слово в столбце - это первое слово в словосочетание, а слово в строке - второе. Подсчитайте вероятность предложений:
    1. *Я хочу пить воду*
    2*. Я хочу играть на гитаре*
    
    ![Снимок экрана 2023-07-17 в 11.37.28.png](%D0%AF%D0%B7%D1%8B%D0%BA%D0%BE%D0%B2%D0%B0%D1%8F%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C%20%D0%BD-%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D1%8B%20fdfd1f68f7894d39b24314d9f07d7d73/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2023-07-17_%25D0%25B2_11.37.28.png)
    
    ![Снимок экрана 2023-07-31 в 17.15.28.png](%D0%AF%D0%B7%D1%8B%D0%BA%D0%BE%D0%B2%D0%B0%D1%8F%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C%20%D0%BD-%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D1%8B%20fdfd1f68f7894d39b24314d9f07d7d73/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2023-07-31_%25D0%25B2_17.15.28.png)
    

### Генерация текста

Идея генерации текста совпадает с основной идеей оценки вероятности предложения: выбор слова зависит от вероятности его появления в данном контексте. То есть для каждого токена в словаре мы считаем вероятность его появления в контексте и выбираем наиболее вероятное слово.

![Снимок экрана 2023-07-17 в 12.23.16.png](%D0%AF%D0%B7%D1%8B%D0%BA%D0%BE%D0%B2%D0%B0%D1%8F%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C%20%D0%BD-%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D1%8B%20fdfd1f68f7894d39b24314d9f07d7d73/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2023-07-17_%25D0%25B2_12.23.16.png)

Другими словами выбор следующего токена из множества возможных можно назвать **сэмплированием.**

Существует несколько методов выбора подходящего слова при генерации:

1. **Жадный метод**
Выбирается наиболее вероятное слово в словаре
Проблема: в основном получаются тексты, состоящие из наиболее часто встречаемых слов, то есть неразнообразные.
2. **Метод k-слов**
Слово выбирается случайным образом из k-наиболее встречаемых слов. Проблема может быть в том, что для н-граммы среди k-наиболее встречаемых слов могут попасться слова с очень низкой вероятностью, например, наиболее частое слово имеет вероятность 0.4, а остальные 0.05. Поэтому с размером k нужно экспериментировать.
3. **Beam Search**
На первом шаге генерируются слова, из которых выбираются k наиболее вероятных. Дальше, для каждого выбранного слова генерируются продолжения и из всех полученных последовательностей выбираются новые k наиболее вероятных. Выбор происходит с помощью расчёта вероятности полученной последовательности слов (формула 1)
    
    ![Снимок экрана 2023-07-24 в 14.49.44.png](%D0%AF%D0%B7%D1%8B%D0%BA%D0%BE%D0%B2%D0%B0%D1%8F%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C%20%D0%BD-%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D1%8B%20fdfd1f68f7894d39b24314d9f07d7d73/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2023-07-24_%25D0%25B2_14.49.44.png)
    

Можно заметить, что при произведении вероятностей каждый раз полученной число становится всё меньше, что значит, что подобный метод будет выбирать более короткие предложения. Чтобы избежать этого можно перевести вероятности в логарифмическую шкалу:

$$
\frac{1}{L^\alpha}logP(y_1, y_2, ..., y_L|<bos>) = \frac{1}{L^\alpha}\sum_{t'=1}^LlogP(y^{t'}|y_1, ..., y_{t'}, <bos>)
$$

## Применение

Конечно, сейчас используются более сложные методы генерации текста, чем простой расчёт вероятности на основе корпуса текстов. Но основная идея сохраняется: в результате работы нейронной сети у каждого слова высчитывается также *вероятность*, с которой данное слово встретится в контексте. Поэтому выше описанные методы сэмплирования и оценки предложения так же подходят для нейронных сетей. Подробнее о них в следующих главах.

# Оценивание языковой модели

Языковые модели работают на основе вероятности: чем более высокая вероятность у сгенерированного предложения, тем оно ближе к реальному. Но для оценки работы модели используют не просто вероятности, а более сложные методы.

## Перплексия

$$
PPL(W) = P(w_1, w_2, ..., w_n)^{-\frac{1}{N}} = \sqrt[N]{\frac{1}{P(w_1, w_2, ..., w_n)}}
$$

Данная формула обозначает обратную вероятность тестового набора, нормализованную на количество слов в тестовом корпусе слов. Вспомним, что вероятность предложения - это произведение вероятностей его частей или в версии н-грамм произведение вероятностей всех н-грамм в предложении:

$$
PPL(W) = \sqrt[N]{\Pi_{i=1}^{N}\frac{1}{P(w_i|w_1, ..., w_{i-1})}}\approx\sqrt[N]{\Pi_{i=1}^{N}\frac{1}{P(w_i|w_{i-n+1}, ..., w_{i-1})}}
$$

Например для униграмм формула будет выглядеть так:

$$
PPL(W) = \sqrt[N]{\Pi_{i=1}^{N}\frac{1}{P(w_i)}}
$$

А для биграмм, соответственно:

$$
PPL(W) = \sqrt[N]{\Pi_{i=1}^{N}\frac{1}{P(w_i|w_{i-1})}}
$$

Заметим, что вероятность обратная, следовательно, чем больше вероятность, тем меньше значение дроби, тем меньше значение перплексии. Мы стремимся к тому, чтобы вероятность тестового набора была наибольшей, а значит перплексия - наименьшей. Таким образом, задача обучения языковой модели сводится к минимизации перплексии.

Можно также считать, что перплексия - *взвешенный коэффициент ветвления языка.* Легче всего понять на примере. Представим ребёнка, который научился говорить только три слова: “мама”, “папа”, “брат”, - и он выкрикивает их с равной вероятностью в течение дня: $p = \frac{1}{3}$. Представим, что на следующий день ничего не изменилось, и ребёнок продолжил выкрикивать эти слова с равной вероятностью. Тогда показатель перплексии будет равен:

$$
PPL(W) = \sqrt[N]{\Pi_{i=1}^{N}\frac{1}{P(w_i)}} = \sqrt[N]{\frac{1}{(\frac{1}{3})^{N}}} = \sqrt[N]{3^N} = 3
$$

Можно заметить, что показатель равен 3, что является коэффициентом ветвления языка, то есть количеству возможных исходов. А теперь представим, что ребёнок стал наиболее часто звать маму. Из 10 случаев 8 - это “мама”, а “папа” и “брат” по одному. Пусть в тестовом наборе также ребёнок не изменяет своего поведения. Тогда:

$$
PPL(W) = \sqrt[10]{\frac{1}{(\frac{1}{10})^2*(\frac{8}{10})^8}} = 1.9
$$

Количество слов не изменилось, а вот уверенность модели - да. Коэффициент ветвления так же равен 3, а вот взвешенный коэффициент стал меньше, так как произношение одного слова (мама) более вероятно, чем произношение других. Другими словами, так как распределение слов в наборах одинаково, модель не удивляется тому, что слово “мама” встречается чаще других, а наоборот, выучив такое распределение в тренировочном наборе, готова к этому. 

Проведём последний эксперимент. Пусть ребёнок стал совсем капризным и стал звать звать маму 98 раз за день и по одному разу брата или папу. На следующий день произошло также. Тогда новый коэффициент перплексии:

$$
PPL(W) = \sqrt[100]{\frac{1}{(\frac{1}{100})^2*(\frac{98}{10})^{98}}} = 1.12
$$

И снова: ребёнок знает всё те же три слова, однако теперь наиболее ожидаемое из них - “мама”. То есть коэффициент показывает, насколько модель уверена в следующем исходе: она всё равно ожидает одно из трёх возможных слов, но слово “мама” наиболее ожидаемо, поэтому коэффициент низкий, так как модель редко удивляется (редко встречает слово отличное от “мама”)

Если мы представим, что в один день ребёнок резко сменил поведение и 98 раз крикнул “папа”, а “мама” и “брат” по одному разу. Тогда перплексия станет такой:

$$
PPL(W) = \sqrt[100]{\frac{1}{(\frac{1}{100})^{98}*(\frac{98}{100})^{1} * (\frac{1}{100})^1}} = 95.5
$$

Такое значение можно объяснить так: “модель встретила распределение чисел, отличное от того, которое она выучила на тренировочном датасете”.

### Удобное представление перплексии

Значения вероятностей, которые вычисляются в знаменателе, очень близки к 0, поэтому в результате проивзедения получится настолько близкое к 0 число, что может вызвать ошибку в программе. Для того, чтобы этого избежать, удобно перейти к логарифмической шкале.

$$
ln(P(W)) = ln(\Pi_{i=1}^{N}P(w_{i})) = \sum_{i=1}^N ln(P(w_i))
$$

Чтобы подобное преобразование оказалось эквивалентно изначальной формуле, нужно использовать свойство возведение основания логарифма в степень равную этому логарифму:

$$
PPL(W) = e^{ln(P(W)^{-\frac{1}{N}})} =e^{{-\frac{1}{N}}ln(P(W))}
$$

# Дополнительно: сглаживание

Что делать, если в корпусе ни разу не встречается предложенный контекст или предложенное предложение? Другими словами, что если в формуле 2.2 знаменатель и/или числитель оказываются равны 0? Ведь если предложение не встретилось в корпусе, это не значит, что оно невозможно. Для разрешения этой проблемы было придумано ***сглаживание***, которое позволяет предположить какую-то оценку для последовательности на основе того, что мы уже видели. 

1. **Backoff**
Если нужная последовательность не встречается в корпусе, можно посчитать встречаемость последовательности на одно слово меньше. Например, если нет необходимой триграммы, то возможно найдется биграмма. Если нет биграммы, то можно попробовать униграмму.
2. **Линейная интерполяция**
Вместо того, чтобы считать отдельно встречаемость какой-то н-граммы, можно представить встречаемость последовательности как сумма вероятностей каждой 
н-граммы с некоторыми весами

$$
P(Я\;люблю\;гладить\;котиков) = \lambda_1*P(котиков|Я\;люблю\;гладить) + \lambda_2*P(гладить|Я\;люблю) +\lambda_3*P(люблю|Я) + \lambda_4*P(Я|<bos>)
$$

1. **Сглаживание Лапласа**
Можно считать, что каждую последовательность мы хотя бы $\alpha$ раз видели в корпусе. Тогда формула расчета вероятности будет выглядеть так:
    
    $$
    P(w_n|w_{n-1}) = \frac{N(w_{n-N+1:n-1}, w_n) + \alpha}{N(w_{n-N+1:n-1}) + \alpha*V} \;
    $$
    
    V в данном случае - это количество слов в словаре, так как при данном контексте мы можем встретить любое слово.
    

# Итог

**Перплексия** - метрика оценивания работы генеративной текстовой модели, которая позволяет оценить, как хорошо модель выучила язык.

Обычно используют преобразованную формулу перплексии, которая включает в себя логарифмы, так как основная формула основана на произведении множества очень маленьких чисел - вероятностей, что может вызвать ошибку в программе.